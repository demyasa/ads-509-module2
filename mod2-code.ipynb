{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1cd3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commands used to install packages not in environment\n",
    "\n",
    "# !pip3 install emoji nltk pprintpp\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# %conda install -c conda-forge textacy\n",
    "# !pip3 install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import regex as re   # stack overflow suggestion for REGEX error encountered\n",
    "\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "\n",
    "import textacy.preprocessing as tprep\n",
    "from lexical_diversity import lex_div as ld\n",
    "import spacy\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import string\n",
    "from pprintpp import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change `data_location` to the location of the folder on your machine.\n",
    "\n",
    "data_location = \"/Users/dunya/Desktop/mod2\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "\n",
    "twitter_folder = f\"{data_location}/twitter\"\n",
    "\n",
    "lyrics_folder = f\"{data_location}/lyrics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0251443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to tokenize text\n",
    "\n",
    "# The following expression matches tokens consisting of at least one letter (\\p{L}), \n",
    "# preceded and followed by an arbitrary sequence of alphanumeric characters \n",
    "# (\\w includes digits, letters, and underscore) and hyphens (-):\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"[\\w-]*\\p{L}[\\w-]*\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for obtaining descriptive statistics\n",
    "\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(tokens, str):     # get tokens\n",
    "        tokens = tokenize(tokens) \n",
    "    else:\n",
    "        tokens = tokens      \n",
    "    \n",
    "    x = np.array(tokens)            # convert token list to np array for unique function\n",
    "    num_uniq = len(np.unique(x))\n",
    "    \n",
    "    test_chars = \"\".join(tokens)    # join list strings for char count\n",
    "\n",
    "    # Lexical diversity\n",
    "    ma_lex_div = ld.mattr(tokens)             # moving average\n",
    "    ttr_lex_div = num_uniq / len(tokens)      # type-token ratio \n",
    "\n",
    "    # Most common tokens (top 5)\n",
    "    common_words = Counter(tokens).most_common(5)    # count token instances then get most common\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = len(tokens)\n",
    "    num_unique_tokens = len(np.unique(x))\n",
    "    lexical_diversity = ttr_lex_div\n",
    "    num_characters = len(test_chars)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity}% in the data.\")\n",
    "        # print the five most common tokens\n",
    "        print(f\"The top 5 words in the data, along with their counts, are {common_words}.\")\n",
    "        \n",
    "    return([num_tokens, \n",
    "            num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters,\n",
    "            common_words])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.6923076923076923% in the data.\n",
      "The top 5 words in the data, along with their counts, are [('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)].\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\"\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "# assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02) # 0.08 !< 0.02   # commented out to not display the error\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code?\n",
    "\n",
    "*The benefits of assertions statements can be observed by the AssertionError produced in the cell above. Assertions allow us to test how well our code is performing in reality compared to  our expteced results. For example, in the failed assertion above, (1) we may be expecting a lexical diversity of roughly 69% and accept that answers within 2% are viable options; or (2) we want to make sure everything is properly formatted in the outputs produced. In the first scenario the assertion informs us the lexical diversity calculation should be revisited. In the second scenario, we can simply reformat that same output by dividing it by 100 or changing 0.69 and 0.02 to whole number representations of the percentages. Long story short, assertion statements help keep our work on track.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d70801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "\n",
    "os.listdir(lyrics_folder) \n",
    "  \n",
    "# Helper to read text file\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()\n",
    "   \n",
    "# Function to iterate through all files\n",
    "\n",
    "def load_songs(artist):\n",
    "    path = f\"{lyrics_folder}/{artist}/\"                          # designate path\n",
    "    os.chdir(path)                                               # change to path directory\n",
    "    song_list = []\n",
    "    for file in os.listdir():\n",
    "        if file.endswith(\".txt\"):                                # check for text format\n",
    "            songs_dict = {}\n",
    "            file_path = f\"{path}{file}\"\n",
    "            path_components = file_path.split('/')\n",
    "            songs_dict[\"Title\"] = path_components[-1]            # get title from file path\n",
    "            songs_dict[\"Artist\"] = path_components[-2]           # get artist from file path\n",
    "            songs_dict[\"Lyrics\"] = read_text_file(file_path)     # extract lyrics\n",
    "            song_list.append(songs_dict)\n",
    "    return song_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "666aadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the lyrics data using load_songs function above\n",
    "\n",
    "artist_names = [\"eminem\", \"adele\"]\n",
    "\n",
    "lyrics_dict = {}\n",
    "for artist in artist_names:\n",
    "    lyric_list = load_songs(artist)\n",
    "    art = lyric_list[0][\"Artist\"]\n",
    "    lyrics_dict[art] = lyric_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data\n",
    "\n",
    "os.listdir(twitter_folder) \n",
    "\n",
    "# Helper to get artist from file name\n",
    "\n",
    "def get_artist(str):\n",
    "    return str.split('_')[0]\n",
    "\n",
    "# Function to load tweets\n",
    "\n",
    "def load_tweets(file_strings):\n",
    "    tweet_dict = {}\n",
    "    for tf in file_strings:\n",
    "        path = f\"{twitter_folder}/{tf}\"     # designate song path\n",
    "        artist = get_artist(tf)\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            # columns = ['screen_name','name', 'id', 'location', 'followers_count','friends_count', 'description']\n",
    "            tweet_df = pd.read_table(path, usecols =['description'])        # extract description values\n",
    "            descs = tweet_df['description'].astype(str).values.tolist()\n",
    "        tweet_dict[artist] = descs\n",
    "    return tweet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20b0adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the twitter data using the load_tweets function above\n",
    "\n",
    "tweet_files = [\"cher_followers_data.csv\", \n",
    "               \"robynkonichiwa_followers_data.csv\"]\n",
    "\n",
    "tweet_dict = load_tweets(tweet_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92863cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to remove punctuation, make lowercase, split on whitespace, and remove stop words\n",
    "\n",
    "def clean_data(text):\n",
    "    punc = string.punctuation                                    # load punctuation\n",
    "    new_punctuation = \"!\\\"$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"         # altered punctuation to keep hashtag\n",
    "    stop_words = spacy.lang.en.STOP_WORDS                        # load stop words\n",
    "\n",
    "    potential_stop_words = [ '', 'im', 'like',                   \n",
    "                            'dont', 'got', 'cause',              # added extra stop words after first results\n",
    "                            'wanna', 'youre']\n",
    "    \n",
    "    for wrd in potential_stop_words:\n",
    "        stop_words.add(wrd)\n",
    "\n",
    "    depunct = text.translate(str.maketrans('', '', new_punctuation))    # remove punctuation\n",
    "    word_list = depunct.casefold().split(\" \")                           # lowercase and split\n",
    "    clean_words = [w for w in word_list if not w in stop_words]         # remove stop words\n",
    "    return clean_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a021c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = spacy.lang.en.STOP_WORDS                        # load stop words\n",
    "\n",
    "# pprint(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f443ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean tweets\n",
    "\n",
    "def clean_tweets(dictionary):\n",
    "    test_dict = {}\n",
    "    for k, v in dictionary.items():         # iterate over each artist\n",
    "        clean_tweet_dict = {}\n",
    "        values = v\n",
    "        clean_tweets = []\n",
    "        for val in values:\n",
    "            new_val = clean_data(val)       # clean description\n",
    "            clean_tweets.append(new_val)    # store word list from description in array\n",
    "\n",
    "        clean_tweet_dict[\"Artist\"] = k\n",
    "        clean_tweet_dict[\"Descriptions\"] = values\n",
    "        tokes = np.concatenate( clean_tweets , axis=0 )     # flatten clean_tweets list\n",
    "        clean_tweet_dict[\"Tokens\"] = tokes\n",
    "        test_dict[k] = clean_tweet_dict     # store new information for each artist\n",
    "    return test_dict\n",
    "\n",
    "cleaned_tweets = clean_tweets(tweet_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c20930ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to remove \\n in lyrics\n",
    "\n",
    "def remove_newlines(value):\n",
    "    return ' '.join(value.splitlines())\n",
    "\n",
    "# Function to clean lyrics\n",
    "\n",
    "def clean_lyrics(dictionary):\n",
    "    tokenize_inputs = []\n",
    "    for art in dictionary:\n",
    "        dusted_lyrics = remove_newlines(art[\"Lyrics\"])       # call helper above\n",
    "        cleaned_lyrics = clean_data(dusted_lyrics)           # use clean_data function previously defined\n",
    "        tokenize_inputs.append(cleaned_lyrics)\n",
    "    \n",
    "    flattened = np.concatenate( tokenize_inputs , axis=0 )   # flatten list elements in tokenize_inputs\n",
    "    return flattened\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "There are 7481 tokens in the data.\n",
      "There are 2867 unique tokens in the data.\n",
      "There are 39034 characters in the data.\n",
      "The lexical diversity is 0.38323753508889186% in the data.\n",
      "The top 5 words in the data, along with their counts, are [('fuck', 83), ('know', 72), ('man', 61), ('aint', 59), ('shit', 54)].\n",
      "[7481, 2867, 0.38323753508889186, 39034, [('fuck', 83), ('know', 72), ('man', 61), ('aint', 59), ('shit', 54)]]\n",
      "\n",
      "\n",
      "There are 3116 tokens in the data.\n",
      "There are 817 unique tokens in the data.\n",
      "There are 15628 characters in the data.\n",
      "The lexical diversity is 0.2621951219512195% in the data.\n",
      "The top 5 words in the data, along with their counts, are [('love', 105), ('rumour', 64), ('ill', 52), ('know', 50), ('heart', 50)].\n",
      "[3116, 817, 0.2621951219512195, 15628, [('love', 105), ('rumour', 64), ('ill', 52), ('know', 50), ('heart', 50)]]\n",
      "\n",
      "\n",
      "There are 16759908 tokens in the data.\n",
      "There are 1662233 unique tokens in the data.\n",
      "There are 98891121 characters in the data.\n",
      "The lexical diversity is 0.09917912437228175% in the data.\n",
      "The top 5 words in the data, along with their counts, are [('nan', 1953910), ('love', 212947), ('life', 122112), ('music', 86403), ('de', 73319)].\n",
      "[16759908, 1662233, 0.09917912437228175, 98891121, [('nan', 1953910), ('love', 212947), ('life', 122112), ('music', 86403), ('de', 73319)]]\n",
      "\n",
      "\n",
      "There are 1595010 tokens in the data.\n",
      "There are 266182 unique tokens in the data.\n",
      "There are 9637463 characters in the data.\n",
      "The lexical diversity is 0.16688422016162907% in the data.\n",
      "The top 5 words in the data, along with their counts, are [('nan', 164845), ('music', 14820), ('love', 11595), ('och', 7922), ('life', 7334)].\n",
      "[1595010, 266182, 0.16688422016162907, 9637463, [('nan', 164845), ('music', 14820), ('love', 11595), ('och', 7922), ('life', 7334)]]\n"
     ]
    }
   ],
   "source": [
    "# Calls to descriptive_stats here\n",
    "\n",
    "# Gather list for eminem_lyrics\n",
    "\n",
    "eminem_lyrics = lyrics_dict[\"eminem\"]\n",
    "eminem_tokens = clean_lyrics(eminem_lyrics)\n",
    "print(\"\\n\")\n",
    "print(descriptive_stats(eminem_tokens, verbose=True))\n",
    "\n",
    "# Gather list for adele_lyrics\n",
    "\n",
    "adele_lyrics = lyrics_dict[\"adele\"]\n",
    "adele_tokens = clean_lyrics(adele_lyrics)\n",
    "print(\"\\n\")\n",
    "print(descriptive_stats(adele_tokens, verbose=True))\n",
    "\n",
    "# Gather list for eminem_tweets\n",
    "\n",
    "cher_tokens = cleaned_tweets[\"cher\"][\"Tokens\"]\n",
    "print(\"\\n\")\n",
    "print(descriptive_stats(cher_tokens, verbose=True))\n",
    "\n",
    "# Gather list for adele_tweets\n",
    "\n",
    "robyn_tokens = cleaned_tweets[\"robynkonichiwa\"][\"Tokens\"]\n",
    "print(\"\\n\")\n",
    "print(descriptive_stats(robyn_tokens, verbose=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: If the stopwords were left in the data the top 5 words would most likely consist of them. Stopwords such as \"and\", \"I\", and \"the\" are some of the most commonly used words in the English language. Leaving them in the data would definitely skew the the top words in their favor. In the output above the word \"ill\" is considered to be one of the top words in Adele's song lyrics. It may be due to the punctuation removed prior to removing stopwords from the data, making it difficult to differentiate whether that top word represents \"I'll\" or \"ill\". It is highly unlikely Adele uses that word so frequently in her music unless it is in the chorus of one of the songs.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: Granted that the genres of the two artists greatly differ (one being rap and the other pop) and Eminem is also kind of considered to be a human dictionary when it comes to his music, the lexical diversity between the two artists conforms to my prior beliefs. I did expect the lexical diversity to be much higher for Eminem though.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "269cd433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher Emojis: \n",
      "\n",
      "[\n",
      "    ('❤️', 14572),\n",
      "    ('🏳️\\u200d🌈', 14008),\n",
      "    ('♥', 10083),\n",
      "    ('❤', 9569),\n",
      "    ('✨', 8278),\n",
      "    ('🌈', 5426),\n",
      "    ('🇺🇸', 3671),\n",
      "    ('💙', 3657),\n",
      "    ('💜', 3481),\n",
      "    ('🌊', 3252),\n",
      "]\n",
      "Robyn Emojis: \n",
      "\n",
      "[\n",
      "    ('🏳️\\u200d🌈', 1682),\n",
      "    ('♥', 1163),\n",
      "    ('❤️', 977),\n",
      "    ('✨', 748),\n",
      "    ('❤', 647),\n",
      "    ('🌈', 563),\n",
      "    ('🎶', 270),\n",
      "    ('🎧', 211),\n",
      "    ('🖤', 210),\n",
      "    ('💜', 205),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 10 most common emojis by artist via twitter descriptions\n",
    "\n",
    "# Function for counting emojis\n",
    "\n",
    "def emoji_counter(token_arr):\n",
    "    word_counts = Counter(token_arr).most_common()      # obtain word counts\n",
    "    emojis = []\n",
    "    for tup in word_counts:\n",
    "        if emoji.is_emoji(tup[0]):      # use emoji library to check tuples\n",
    "            emojis.append(tup)          # store if emoji\n",
    "    return emojis[0:10]\n",
    "\n",
    "cher_em = emoji_counter(cher_tokens)\n",
    "rob_em = emoji_counter(robyn_tokens)\n",
    "\n",
    "print(\"Cher Emojis: \\n\")\n",
    "pprint(cher_em)\n",
    "print(\"Robyn Emojis: \\n\")\n",
    "pprint(rob_em)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher Hashtags: \n",
      "\n",
      "[\n",
      "    ('#resist', 10488),\n",
      "    ('#blm', 9542),\n",
      "    ('#blacklivesmatter', 7700),\n",
      "    ('#theresistance', 3199),\n",
      "    ('#fbr', 3123),\n",
      "    ('#resistance', 2770),\n",
      "    ('#1', 2419),\n",
      "    ('#', 2122),\n",
      "    ('#voteblue', 2047),\n",
      "    ('#lgbtq', 1786),\n",
      "]\n",
      "Robyn Hashtags: \n",
      "\n",
      "[\n",
      "    ('#blacklivesmatter', 580),\n",
      "    ('#blm', 338),\n",
      "    ('#music', 289),\n",
      "    ('#1', 194),\n",
      "    ('#', 168),\n",
      "    ('#teamfollowback', 127),\n",
      "    ('#edm', 108),\n",
      "    ('#lgbtq', 81),\n",
      "    ('#resist', 78),\n",
      "    ('#art', 69),\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 10 most common hashtags by artist via twitter descriptions\n",
    "\n",
    "# Function for counting hashtags\n",
    "\n",
    "def hashtag_counter(token_arr):\n",
    "    word_counts = Counter(token_arr).most_common()      # obtain word counts\n",
    "    hashtags = []\n",
    "    for tup in word_counts:\n",
    "        token = tup[0]              # extract word in tuple\n",
    "        if token[0] == \"#\":         # check if first part of word string is hashtag\n",
    "            hashtags.append(tup)    # store if hashtag\n",
    "    return hashtags[0:10]\n",
    "    \n",
    "cher_hash = hashtag_counter(cher_tokens)\n",
    "rob_hash = hashtag_counter(robyn_tokens)\n",
    "\n",
    "print(\"Cher Hashtags: \\n\")\n",
    "pprint(cher_hash)\n",
    "print(\"Robyn Hashtags: \\n\")\n",
    "pprint(rob_hash)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adele Common Title Words:\n",
      " [('You', 4), ('My', 4), ('It', 3), ('For', 3), ('Love', 3)]\n",
      "Eminem Common Title Words:\n",
      " [('(Skit)', 5), ('Murder', 2), ('Just', 2), ('My', 2), ('W.E.G.O.', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 5 most common words in song titles by artist\n",
    "\n",
    "adele_songs = lyrics_dict[\"adele\"]\n",
    "eminem_songs = lyrics_dict[\"eminem\"]\n",
    "\n",
    "# Function for finding common words in title\n",
    "\n",
    "def common_words_in_title(arr):\n",
    "    titles = []\n",
    "    for el in arr:\n",
    "        title = el[\"Title\"]                 # extract title\n",
    "        title = title.replace(\".txt\", \"\")   # remove .txt\n",
    "        title = title.split(\"_\")            # split titles into word lists\n",
    "        titles.append(title)\n",
    "    \n",
    "    flattened = np.concatenate( titles , axis=0 )       # flatten lists in titles list\n",
    "\n",
    "    title_counts = Counter(flattened).most_common(5)    # get top 5 words\n",
    "    return title_counts\n",
    "\n",
    "adele_titles = common_words_in_title(adele_songs)\n",
    "print(\"Adele Common Title Words:\\n\", adele_titles)\n",
    "\n",
    "eminem_titles = common_words_in_title(eminem_songs)\n",
    "print(\"Eminem Common Title Words:\\n\", eminem_titles)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Artist 1    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Artist 2    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of tokens for number of tokens in each song\n",
    "\n",
    "num_replicates = 1000\n",
    "\n",
    "df_ex = pd.DataFrame({\n",
    "    \"artist\" : ['Artist 1'] * num_replicates + ['Artist 2']*num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(125,num_replicates),np.random.poisson(150,num_replicates)))\n",
    "})\n",
    "\n",
    "df_ex.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: The `'s'` character represents a space, while the `'+'` indicates more than one. Therefore the regular expression `'\\s+'` matches on any part of text that consists of one or more spaces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2294c440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Adele     AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Eminem    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your lyric length comparison chart here. \n",
    "\n",
    "df_plot = pd.DataFrame({\n",
    "    \"artist\" : ['Eminem'] * num_replicates + ['Adele'] * num_replicates,\n",
    "    \"length\" : np.concatenate((np.random.poisson(len(eminem_tokens),num_replicates),np.random.poisson(len(adele_tokens),num_replicates)))\n",
    "})\n",
    "\n",
    "df_plot.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
